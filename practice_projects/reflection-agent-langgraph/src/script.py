import os
from langchain_openai import OpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.graph import END, StateGraph

from typing import List, Annotated, TypedDict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

llm = OpenAI(
    model_name="gpt-4o-mini",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY"),
)

# Reflection is a prompting strategy aimed at enhancing the quality and accuracy of outputs generated by AI agents.
# It involves getting the agent to pause, review, and critique its own outputs before finalizing them.
# This iterative process helps in reducing errors and improving performance over time.

generation_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a professional LinkedIn content assistant tasked with crafting engaging, insightful, and well-structured LinkedIn posts."
            " Generate the best LinkedIn post possible for the user's request."
            " If the user provides feedback or critique, respond with a refined version of your previous attempts, improving clarity, tone, or engagement as needed.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

generate_chain = generation_prompt | llm

reflection_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """You are a professional LinkedIn content strategist and thought leadership expert. Your task is to critically evaluate the given LinkedIn post and provide a comprehensive critique. Follow these guidelines:

        1. Assess the postâ€™s overall quality, professionalism, and alignment with LinkedIn best practices.
        2. Evaluate the structure, tone, clarity, and readability of the post.
        3. Analyze the postâ€™s potential for engagement (likes, comments, shares) and its effectiveness in building professional credibility.
        4. Consider the postâ€™s relevance to the authorâ€™s industry, audience, or current trends.
        5. Examine the use of formatting (e.g., line breaks, bullet points), hashtags, mentions, and media (if any).
        6. Evaluate the effectiveness of any call-to-action or takeaway.

        Provide a detailed critique that includes:
        - A brief explanation of the postâ€™s strengths and weaknesses.
        - Specific areas that could be improved.
        - Actionable suggestions for enhancing clarity, engagement, and professionalism.

        Your critique will be used to improve the post in the next revision step, so ensure your feedback is thoughtful, constructive, and practical.
        """
    ),
    MessagesPlaceholder(variable_name="messages")
])

reflect_chain = reflection_prompt | llm

# Agent State for Reflection Agent

# Define state with TypedDict
class AgentState(TypedDict):
    messages: Annotated[List[HumanMessage | AIMessage | SystemMessage], "add_messages"]
    version: int  # Tracks the version of the LinkedIn post (increments with each generation)

# AIMessage - to represent messages from the AI
# HumanMessage - to represent messages from the human user
# SystemMessage - to represent system-level messages
# "add_messages" - action to add messages to the state

# Instead of manually managing state updates, we use StateGraph with the AgentState we defined above.
# StateGraph manages the state transitions and message history automatically.
# Features of StateGraph with messages key:
# - Structured state management with type safety.
# - Built-in actions for adding and retrieving messages.
# - Seamless integration with language models for generating and reflecting on messages.

graph = StateGraph(AgentState)

# Generation and reflection node

def generation_node(state: AgentState) -> AgentState:
    generated_post = generate_chain.invoke({"messages": state["messages"]})
    # OpenAI returns a string directly, not an object with .content
    content = generated_post if isinstance(generated_post, str) else generated_post.content

    # Increment version counter
    current_version = state.get("version", 0)
    new_version = current_version + 1

    print(f"\nðŸ”„ Generating LinkedIn Post - Version {new_version}")

    return {"messages": [AIMessage(content=content)], "version": new_version}

def reflection_node(state: AgentState) -> AgentState:
    current_version = state.get("version", 0)
    print(f"\nðŸ’­ Reflecting on Version {current_version}")

    res = reflect_chain.invoke({"messages": state["messages"]})  # Passes messages as input to reflect_chain
    # OpenAI returns a string directly, not an object with .content
    content = res if isinstance(res, str) else res.content
    return {"messages": [HumanMessage(content=content)]}  # Returns the refined message as HumanMessage for feedback

# Why HumanMessage in the reflection_node?
# The output is wrapped in a HumanMessage because the reflection process is a form of feedback or critique given to the generation agent,
# and the feedback is intended to be treated as if it is coming from the user. 

graph.add_node("generate", generation_node)
graph.add_node("reflect", reflection_node)
graph.add_edge("reflect", "generate")
graph.set_entry_point("generate")

def should_continue(state: AgentState):
    version = state.get("version", 0)
    max_versions = 3  # Maximum number of versions to generate

    print(f"\nðŸ“Š Status Check - Version {version}/{max_versions}")
    print("=" * 70)

    # Stop after reaching max versions
    if version >= max_versions:
        print(f"âœ… Reached maximum versions ({max_versions}). Ending workflow.\n")
        return END

    print(f"ðŸ” Continuing to reflection and next generation...\n")
    return "reflect"

graph.add_conditional_edges("generate", should_continue)

workflow = graph.compile()

inputs = {
    "messages": [HumanMessage(content="""Write a linkedin post on 'The different type of AI Agents' under 160 characters""")],
    "version": 0  # Start with version 0, will increment to 1 on first generation
}

print("ðŸš€ Starting LinkedIn Post Generation with Reflection Agent")
print("=" * 70)

response = workflow.invoke(inputs)

# Access messages from the final state
messages = response["messages"]
final_version = response.get("version", 0)

print("\n" + "=" * 70)
print(f"ðŸŽ‰ Workflow Complete! Final Version: {final_version}")
print(f"ðŸ“¨ Total Messages: {len(messages)}")
print("=" * 70)

# Find the first AI-generated post (should be index 1)
if len(messages) > 1:
    print("\nðŸ“ FIRST GENERATED POST (Version 1):")
    print("-" * 70)
    print(messages[1].content)

# Show first reflection if it exists
if len(messages) > 2:
    print("\nðŸ’­ FIRST REFLECTION:")
    print("-" * 70)
    print(messages[2].content)

# Show the final AI message (last generated post)
if len(messages) > 0:
    # Find the last AIMessage in the list
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break

    if last_ai_message:
        print(f"\nâœ¨ FINAL POST (Version {final_version}):")
        print("-" * 70)
        print(last_ai_message.content)
        print("=" * 70)