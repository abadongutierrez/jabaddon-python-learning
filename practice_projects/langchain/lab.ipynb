{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5334e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18e4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c85ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQOsec3EuldyMObmXzYmX9eCtuTLM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c5c1b844-85de-4bec-a5b1-d49a4c0b86ab-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f689e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea4f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314c8b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9744088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d281021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea67db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab5524a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The day after Tuesday is Wednesday.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 23, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CQOx161SvGKykIbWJq10Rkfq1q4ne', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c45e8e03-ff8b-4e64-8389-5d80ae4e9709-0' usage_metadata={'input_tokens': 23, 'output_tokens': 7, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "response = chain.invoke(input = input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da4282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "217ffe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence\"),\n",
    "        HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab0db6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You should read \"The Girl with the Dragon Tattoo\" by Stieg Larsson for its gripping plot and complex characters.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 42, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQOxVgzlvzTZZRjGmkHZpHoDSEfvI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9816da8f-eede-4b24-991c-38ee70d02fc5-0' usage_metadata={'input_tokens': 42, 'output_tokens': 24, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a38ad9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a076beaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 3 to 5 times a week for optimal results!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQOy112mQhr2D9pjUAPxwDwA6EiVT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b3eb10ff-a6f1-43b7-ab6e-a7ae6b3465f9-0' usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c43ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What month follows June?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c00e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The month that follows June is July.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 12, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQOyLeMQL3OvxKjwRiMmNv4wPTITU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--87c91df3-d323-4489-b440-815784999f12-0' usage_metadata={'input_tokens': 12, 'output_tokens': 8, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40f15",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b349be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6467877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de06d6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "303d13c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba7f677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68da0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The day after Tuesday is Wednesday.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 23, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CQOznp2RTPI9MMMbwiJq0yHtrAcrJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e2516801-b1f2-461a-8b24-05f9bc98e76c-0' usage_metadata={'input_tokens': 23, 'output_tokens': 7, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "response = chain.invoke(input = input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65356b97",
   "metadata": {},
   "source": [
    "### Example selectors\n",
    "\n",
    "If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.\n",
    "\n",
    "Example selector types could based on:\n",
    "- `Similarity`: Uses semantic similarity between inputs and examples to decide which examples to choose.\n",
    "- `MMR`: Uses Max Marginal Relevance between inputs and examples to decide which examples to choose.\n",
    "- `Length`: Selects examples based on how many can fit within a certain length\n",
    "- `Ngram`: Uses ngram overlap between inputs and examples to decide which examples to choose.\n",
    "\n",
    "Here, you can use the example selector based on length as an example. For more details on other types, please refer to [https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb17af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,  # The maximum length that the formatted examples should be.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eadac867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: big\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbbd92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d268785",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "\n",
    "Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e711957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf4ff127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee57af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4248535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d44384d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanilla',\n",
       " 'chocolate',\n",
       " 'strawberry',\n",
       " 'mint chocolate chip',\n",
       " 'cookies and cream']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea363f",
   "metadata": {},
   "source": [
    "### Documents\n",
    "\n",
    "#### Document object\n",
    "\n",
    "A `Document` object in `LangChain` contains information about some data. It has two attributes:\n",
    "\n",
    "- `page_content`: *`str`*: This attribute holds the content of the document\\.\n",
    "- `metadata`: *`dict`*: This attribute contains arbitrary metadata associated with the document. It can be used to track various details such as the document id, file name, and so on.\n",
    "\n",
    "Let's use an example to illustrate how to create a `Document` object. This is the object type that `LangChain` utilizes for handling text or documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "544748da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef4e1b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language. \\n                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"About Python\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c989ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Python is an interpreted high-level general-purpose programming language. \\n                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2dbf7d",
   "metadata": {},
   "source": [
    "## Document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c98d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e892cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = (\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3a02a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03ad0665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Figure 2. An AIMessage illustration \\nC. Prompt Template \\nPrompt templates [10] allow you to structure input for LLMs. \\nThey provide a convenient way to format user inputs and \\nprovide instructions to generate responses. Prompt templates \\nhelp ensure that the LLM understands the desired context and \\nproduces relevant outputs. \\nThe prompt template classes in LangChain are built to \\nmake constructing prompts with dynamic inputs easier. Of \\nthese classes, the simplest is the PromptTemplate. \\nD. Chain \\nChains [11] in LangChain refer to the combination of \\nmultiple components to achieve specific tasks. They provide \\na structured and modular approach to building language \\nmodel applications. By combining different components, you \\ncan create chains that address various u se cases and \\nrequirements. Here are some advantages of using chains: \\nâ€¢ Modularity: Chains allow you to break down \\ncomplex tasks into smaller, manageable \\ncomponents. Each component can be developed and \\ntested independently, making it easier to maintain \\nand update the application. \\nâ€¢ Simplification: By combining components into a \\nchain, you can simplify the overall implementation \\nof your application. Chains abstract away the \\ncomplexity of working with individual components, \\nproviding a higher-level interface for developers. \\nâ€¢ Debugging: When an issue arises in your \\napplication, chains can help pinpoint the \\nproblematic component. By isolating the chain and \\ntesting each component individually, you can \\nidentify and troubleshoot any errors or unexpected \\nbehavior. \\nâ€¢ Maintenance: Chains make it easier to update or \\nreplace specific components without affecting the \\nentire application. If a new version of a component \\nbecomes available or if you want to switch to a \\ndiffer. \\nTo build a chain, you simply combine the desired components \\nin the order they should be executed. Each component in the \\nchain takes the output of the previous component as input, \\nallowing for a seamless flow of data and interaction with the \\nlanguage model. \\nE. Memory  \\nThe ability to remember prior exchanges conversation is \\nreferred to as memory  [12]. LangChain includes several \\nprograms for increasing system memory. These utilities can \\nbe used independently or as a part of a chain.  We call this \\nability to store information about past interactions \"memory\". \\nLangChain provides a lot of utilities for adding memory to a \\nsystem. These utilities can be used by themselves or \\nincorporated seamlessly into a chain. \\nA memory system must support two fundamental \\nactions: reading and writing. Remember that each chain has \\nsome fundamental execution mechanism that requires \\nspecific inputs. Some of these inputs are provided directly by \\nthe user, while others may be retrieve d from memory. In a \\nsingle run, a chain will interact with its memory system twice. \\n1. A chain will READ from its memory system and \\naugment the user inputs AFTER receiving the initial \\nuser inputs but BEFORE performing the core logic. \\n2. After running the basic logic but before providing the \\nsolution, a chain will WRITE the current run\\'s inputs \\nand outputs to memory so that they may be referred \\nto in subsequent runs. \\nAny memory system\\'s two primary design decisions are: \\n1. How state is stored ? \\nStoring: List of chat messages: A history of all chat \\nexchanges is behind each memory. Even if not all of \\nthese are immediately used, they must be preserved \\nin some manner. A series of integrations for storing \\nthese conversation messages, ranging from in -\\nmemory lists to persistent databases, is a significant \\ncomponent of the LangChain memory module. \\n2. How state is queried ? \\nQuerying: Data structures and algorithms on top of \\nchat messages: Keeping track of chat messages is a \\nsimple task. What is less obvious are the data \\nstructures and algorithms built on top of chat \\nconversations to provide the most usable view of \\nthose chats. \\nA simple memory system may only return the most \\nrecent messages on each iteration. A slightly more \\ncomplicated memory system may return a brief summary of \\nthe last K messages. A more complex system might extract \\nentities from stored messages and only retur n information \\nabout entities that have been referenced in the current run. \\nThere are numerous sorts of memories. Each has its own set \\nof parameters and return types and is helpful in a variety of \\nsituations.  \\nMemory Types:  \\nâ€¢ ConversationBufferMemory allows for saving \\nmessages and then extracts the messages in a \\nvariable. \\nâ€¢ ConversationBufferWindowMemory keeps a list of \\nthe interactions of the conversation over time. It only \\nuses the last K interactions. This can be useful for \\nkeeping a sliding window of the most recent \\ninteractions, so the buffer does not get too large. \\nThe MindGuide chatbot uses conversation buffer memory. \\nThis memory allows for storing messages and then extracts \\nthe messages in a variable. \\nIII. ARCHITETURE \\nIn crafting the architecture of the MindGuide app, each \\nstep is meticulously designed to create a seamless and \\neffective user experience for those seeking mental health \\nsupport. The user interface, built on Streamlit, sets the tone \\nwith a friendly and safe welcome. Users can jump in by typing \\nWelcome! to your therapy session. I\\'m here to listen, \\nsupport, and guide you through any mental health \\nchallenges or concerns you may have. Please feel free \\nto share what\\'s on your mind, and we\\'ll work together \\nto address your needs. Remember, this is a safe and \\nconfidential space for you to express y ourself. Let\\'s \\nbegin when you\\'re ready.')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[2]  # take a look at the page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9ec093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMâ€™s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these appl\n"
     ]
    }
   ],
   "source": [
    "print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95dfc58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf langchain_community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5213ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef259475",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47bb1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db073b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction | ðŸ¦œï¸ðŸ”— LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentA newer LangChain version is out! Check out the latest version.IntegrationsAPI referenceLatestLegacyMorePeopleContributingCookbooks3rd party tutorialsYouTubearXivv0.2Latestv0.2v0.1ðŸ¦œï¸ðŸ”—LangSmithLangSmith DocsLangChain HubJS/TS DocsðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph database\n"
     ]
    }
   ],
   "source": [
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabd518",
   "metadata": {},
   "source": [
    "## Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33ae51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d4d3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")  # define chunk_size which is length of characters, and also separator.\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c74dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content   # take a look at any chunk's page content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31065336",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fea2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01a983ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01320977695286274,\n",
       " 0.015505682677030563,\n",
       " -0.01783486269414425,\n",
       " -0.022260304540395737,\n",
       " -0.020363686606287956]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.page_content for text in chunks]\n",
    "\n",
    "embedding_result = embeddings_model.embed_documents(texts)\n",
    "embedding_result[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ba504",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6df990ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf langchain_community beautifulsoup4 chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43b7c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ff6ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45463d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMâ€™s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other\n"
     ]
    }
   ],
   "source": [
    "query = \"Langchain\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c665950",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Retrievers accept a string `query` as input and return a list of `Document`'s as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "645d38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b2ec359",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20dc8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'total_pages': 6, 'producer': 'PyPDF', 'moddate': '2023-12-31T03:52:06+00:00', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'page': 1, 'creator': 'Microsoft Word', 'page_label': '2', 'creationdate': '2023-12-31T03:50:13+00:00', 'title': 's8329 final', 'author': 'IEEE'}, page_content='LangChain helps us to unlock the ability to harness the \\nLLMâ€™s immense potential in tasks such as document analysis, \\nchatbot development, code analysis, and countless other')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df494a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89ee5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=embeddings_model\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22cd955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b67f7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "68e4a046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8449d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f195254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMâ€™s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2f2645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "71a11720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMâ€™s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these applications can make informed \n",
      "decisions about how to respond based on the provided \n",
      "context and determine the appropriate actions to take. \n",
      "LangChain offers several key value propositions: \n",
      "Modular Components: It provides abstractions that \n",
      "simplify working with language models, along with a \n",
      "comprehensive collection of implementations for each \n",
      "abstraction. These components are designed to be modular \n",
      "and user -friendly, making them useful whethe r you are \n",
      "utilizing the entire LangChain framework or not. \n",
      "Off-the-Shelf Chains: LangChain offers pre -configured \n",
      "chains, which are structured assemblies of components \n",
      "tailored to accomplish specific high -level tasks. These pre -\n",
      "defined chains streamline the initial setup process and serve as \n",
      "an ideal starting point for your projects. The MindGuide Bot \n",
      "uses below components from LangChain. \n",
      "A. ChatModel \n",
      "Within LangChain, a ChatModel is a specific kind of \n",
      "language model crafted to manage conversational\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f7249b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "832682be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is this paper discussing?',\n",
       " 'result': 'The paper is discussing the creation of an advanced solution for early detection and comprehensive support within the field of mental health. It emphasizes the importance of addressing mental disorders, particularly anxiety, depression, and suicidal thoughts, and highlights the need for effective interventions in modern society.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=False)\n",
    "query = \"what is this paper discussing?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31c904",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bab3c",
   "metadata": {},
   "source": [
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53946fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc403dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm \n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5aa4c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of France?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1504ea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQPQ25plQugIOADiUT714irHWAiXM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0dbf7294-8569-42ea-838c-31eab1cc368e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c742e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of France?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQPQ25plQugIOADiUT714irHWAiXM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0dbf7294-8569-42ea-838c-31eab1cc368e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a1b8d",
   "metadata": {},
   "source": [
    "#### Conversation buffer\n",
    "\n",
    "This type of memory allows for the storage of messages, which can then be extracted to a variable. Consider using this in a chain, setting `verbose=True` so that the prompt can be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce3e29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93f23389",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb3df41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Hello, I am a little cat. Who are you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Hello, little cat! I'm an AI here to chat and help you with anything you need. What would you like to talk about today? ðŸ¾\",\n",
       " 'chat_history': [HumanMessage(content='Hello, I am a little cat. Who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello, little cat! I'm an AI here to chat and help you with anything you need. What would you like to talk about today? ðŸ¾\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conversation.invoke(input=\"Hello, I am a little cat. Who are you?\")\n",
    "conversation.invoke({\"text\": \"Hello, I am a little cat. Who are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33d679fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Hello, I am a little cat. Who are you?\n",
      "AI: Hello, little cat! I'm an AI here to chat and help you with anything you need. What would you like to talk about today? ðŸ¾\n",
      "Human: What can you do?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I can do a variety of things! Here are some of them:\\n\\n1. **Answer Questions**: I can provide information on a wide range of topics.\\n2. **Tell Stories**: I can create fun stories or help you come up with ideas for your own.\\n3. **Provide Advice**: I can offer tips on various subjects, from studying to pet care.\\n4. **Engage in Conversation**: I can chat about your interests, hobbies, or anything else on your mind.\\n5. **Play Games**: I can suggest games or even play text-based games with you.\\n\\nWhat would you like to do?',\n",
       " 'chat_history': [HumanMessage(content='Hello, I am a little cat. Who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello, little cat! I'm an AI here to chat and help you with anything you need. What would you like to talk about today? ðŸ¾\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What can you do?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I can do a variety of things! Here are some of them:\\n\\n1. **Answer Questions**: I can provide information on a wide range of topics.\\n2. **Tell Stories**: I can create fun stories or help you come up with ideas for your own.\\n3. **Provide Advice**: I can offer tips on various subjects, from studying to pet care.\\n4. **Engage in Conversation**: I can chat about your interests, hobbies, or anything else on your mind.\\n5. **Play Games**: I can suggest games or even play text-based games with you.\\n\\nWhat would you like to do?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"text\": \"What can you do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228597fa",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f015fa",
   "metadata": {},
   "source": [
    "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.\n",
    "\n",
    "It combines different LLM calls and actions automatically.\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "609ce831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "31f88786",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "                {location}\n",
    "                \n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['location'])\n",
    "\n",
    "# chain 1\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22d38e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'China',\n",
       " 'meal': 'One classic dish from China is Peking Duck. This iconic dish is known for its crispy skin and tender meat, traditionally served with thin pancakes, hoisin sauce, and sliced scallions. It originates from Beijing and is celebrated for its rich flavor and elaborate preparation process.'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_chain.invoke(input={'location':'China'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ace47e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64974064",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['meal'])\n",
    "\n",
    "# chain 2\n",
    "dish_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aa7d02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['recipe'])\n",
    "\n",
    "# chain 3\n",
    "recipe_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f441fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall chain\n",
    "overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],\n",
    "                                      input_variables=['location'],\n",
    "                                      output_variables=['meal', 'recipe', 'time'],\n",
    "                                      verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9dc41122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1b21aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'location': 'China',\n",
      " 'meal': 'One classic dish from China is Peking Duck. This iconic dish is '\n",
      "         'known for its crispy skin and tender meat, traditionally served with '\n",
      "         \"thin pancakes, hoisin sauce, and sliced scallions. It's a celebrated \"\n",
      "         'dish that showcases the culinary artistry of Chinese cuisine, '\n",
      "         'particularly from Beijing.',\n",
      " 'recipe': '### Peking Duck Recipe\\n'\n",
      "           '\\n'\n",
      "           '#### Ingredients:\\n'\n",
      "           '- 1 whole duck (about 5-6 lbs)\\n'\n",
      "           '- 1 tablespoon salt\\n'\n",
      "           '- 1 tablespoon sugar\\n'\n",
      "           '- 1 tablespoon soy sauce\\n'\n",
      "           '- 1 tablespoon rice vinegar\\n'\n",
      "           '- 1 tablespoon honey\\n'\n",
      "           '- 2 cups boiling water\\n'\n",
      "           '- Thin pancakes (store-bought or homemade)\\n'\n",
      "           '- Hoisin sauce\\n'\n",
      "           '- Sliced scallions\\n'\n",
      "           '\\n'\n",
      "           '#### Instructions:\\n'\n",
      "           '\\n'\n",
      "           '1. **Prepare the Duck:**\\n'\n",
      "           '   - Rinse the duck under cold water and pat it dry with paper '\n",
      "           'towels.\\n'\n",
      "           '   - Rub the salt and sugar all over the duck, including the '\n",
      "           'cavity. Let it sit for 30 minutes.\\n'\n",
      "           '\\n'\n",
      "           '2. **Blanch the Duck:**\\n'\n",
      "           '   - In a large pot, bring 2 cups of water to a boil. Carefully '\n",
      "           'pour the boiling water over the duck to tighten the skin. Let it '\n",
      "           'drain and cool for 10 minutes.\\n'\n",
      "           '\\n'\n",
      "           '3. **Glaze the Duck:**\\n'\n",
      "           '   - In a small bowl, mix soy sauce, rice vinegar, and honey. '\n",
      "           'Brush this mixture all over the duck.\\n'\n",
      "           '\\n'\n",
      "           '4. **Dry the Duck:**\\n'\n",
      "           '   - Hang the duck in a cool, dry place (or place it on a rack in '\n",
      "           'the fridge) for at least 4 hours or overnight to dry out the '\n",
      "           'skin.\\n'\n",
      "           '\\n'\n",
      "           '5. **Roast the Duck:**\\n'\n",
      "           '   - Preheat your oven to 375Â°F (190Â°C). Place the duck on a '\n",
      "           'roasting rack in a baking tray.\\n'\n",
      "           '   - Roast for about 1.5 hours, basting occasionally with the '\n",
      "           'drippings, until the skin is crispy and golden brown.\\n'\n",
      "           '\\n'\n",
      "           '6. **Serve:**\\n'\n",
      "           '   - Let the duck rest for 10 minutes before carving. Serve with '\n",
      "           'thin pancakes, hoisin sauce, and sliced scallions.\\n'\n",
      "           '\\n'\n",
      "           'Enjoy your homemade Peking Duck!',\n",
      " 'time': 'To estimate the total time needed to cook Peking Duck based on the '\n",
      "         'provided recipe, we can break it down into the following steps:\\n'\n",
      "         '\\n'\n",
      "         '1. **Preparation Time:** \\n'\n",
      "         '   - Rinsing, drying, and rubbing the duck with salt and sugar: **30 '\n",
      "         'minutes**\\n'\n",
      "         '   \\n'\n",
      "         '2. **Blanching Time:** \\n'\n",
      "         '   - Boiling water and pouring it over the duck: **10 minutes** '\n",
      "         '(including cooling time)\\n'\n",
      "         '\\n'\n",
      "         '3. **Glazing Time:** \\n'\n",
      "         '   - Mixing the glaze and brushing it on the duck: **5 minutes**\\n'\n",
      "         '\\n'\n",
      "         '4. **Drying Time:** \\n'\n",
      "         \"   - Hanging the duck to dry: **4 hours** (or overnight, but we'll \"\n",
      "         'use the minimum time for this estimate)\\n'\n",
      "         '\\n'\n",
      "         '5. **Roasting Time:** \\n'\n",
      "         '   - Preheating the oven and roasting the duck: **1.5 hours** (90 '\n",
      "         'minutes)\\n'\n",
      "         '\\n'\n",
      "         '6. **Resting Time:** \\n'\n",
      "         '   - Letting the duck rest before carving: **10 minutes**\\n'\n",
      "         '\\n'\n",
      "         '### Total Time Estimate:\\n'\n",
      "         '- Preparation: 30 minutes\\n'\n",
      "         '- Blanching: 10 minutes\\n'\n",
      "         '- Glazing: 5 minutes\\n'\n",
      "         '- Drying: 4 hours (240 minutes)\\n'\n",
      "         '- Roasting: 1.5 hours (90 minutes)\\n'\n",
      "         '- Resting: 10 minutes\\n'\n",
      "         '\\n'\n",
      "         '**Total Time = 30 + 10 + 5 + 240 + 90 + 10 = 385 minutes**\\n'\n",
      "         '\\n'\n",
      "         'This translates to approximately **6 hours and 25 minutes**. \\n'\n",
      "         '\\n'\n",
      "         'Keep in mind that if you choose to dry the duck overnight, the total '\n",
      "         'time will increase significantly, but the active cooking time '\n",
      "         'remains the same. Enjoy your cooking!'}\n"
     ]
    }
   ],
   "source": [
    "pprint(overall_chain.invoke(input={'location':'China'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b1376d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "49f8fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)\n",
    "response = chain.invoke(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "78d71080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for developing applications powered by large language models (LLMs). It streamlines the entire application lifecycle, including development, productionization, and deployment. Key components include langchain-core for base abstractions, langchain-community for third-party integrations, LangGraph for building stateful applications, LangServe for deploying APIs, and LangSmith for monitoring and evaluating LLM applications. The documentation offers tutorials, how-to guides, and conceptual overviews to assist users in building various applications, such as chatbots and question-answering systems. LangChain is part of a broader ecosystem with numerous integrations and is actively maintained with ongoing updates.\n"
     ]
    }
   ],
   "source": [
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27ba6c",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "##### Tools\n",
    "\n",
    "Tools are interfaces that an agent, a chain, or a chat model / LLM can use to interact with the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9721ee",
   "metadata": {},
   "source": [
    "You can find a list of tools that LangChain supports at [https://python.langchain.com/v0.1/docs/integrations/tools/](https://python.langchain.com/v0.1/docs/integrations/tools/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015a967",
   "metadata": {},
   "source": [
    "Letâ€™s explore how to work with tools, using the `Python REPL` tool as an example. The `Python REPL` tool can execute Python commands. These commands can either come from the user or be generated by the LLM. This tool is particularly useful for complex calculations. Instead of having the LLM generate the answer directly, it can be more efficient to have the LLM generate code to calculate the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "380ca8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7ee424b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8871206b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repl.run(\"a = 3; b = 1; print(a+b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047fc51",
   "metadata": {},
   "source": [
    "##### Toolkits\n",
    "\n",
    "Toolkits are collections of tools that are designed to be used together for specific tasks.\n",
    "\n",
    "Let's create a toolkit that contains one tool which is `PythonREPLTool`. Note that tools are put into a `list` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f0d97b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "tools = [PythonREPLTool()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dc2b1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc94f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "# here you will use the prompt directly from the langchain hub\n",
    "base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3fada020",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)  # tools were defined in the toolkit part above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "da89f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mI need to debug the code to correctly calculate the 3rd Fibonacci number. The Fibonacci sequence starts with 0, 1, 1, 2, 3, ... so the 3rd Fibonacci number (considering 0-based indexing) is 2.\n",
      "\n",
      "Let's run the code again to confirm this. \n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(2)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mI made an error in my previous reasoning about the Fibonacci sequence. The 3rd Fibonacci number in a 0-based index is indeed 2, but I need to confirm this by running the code correctly.\n",
      "\n",
      "Let's run the code again to find the 3rd Fibonacci number using 0-based indexing.\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mI don't know\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Python_REPL\n",
      "Action Input: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2); fibonacci(3)\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the 3rd fibonacci number?',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input = {\"input\": \"What is the 3rd fibonacci number?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
